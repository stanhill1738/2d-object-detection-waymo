{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "845a8429-5faa-45f7-a9f2-c671a326aeeb",
   "metadata": {},
   "source": [
    "# Training a Faster R-CNN model for 2D Object Detection on Waymo Perception Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577ad00c-9aa6-40ba-89c8-71fdd133f78e",
   "metadata": {},
   "source": [
    "This notebook walks through the steps required in order to train and evaluate a Faster R-CNN model using Waymo Perception Open Dataset, for the purpose of 2D Object Detection - i.e bounding boxes for AV images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cffed20-053f-437d-86c9-6ecae7597dae",
   "metadata": {},
   "source": [
    "## 1. Explore the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd26123-7bc5-494a-afaa-0d4c6c1b8f31",
   "metadata": {},
   "source": [
    "The data that we are using comes from the [Waymo Perception Open Dataset](https://waymo.com/open/data/perception/).\n",
    "You will need to fill out a form to gain access to this data.\n",
    "\n",
    "The dataset itself lives in a Google Cloud Storage (GCS) bucket, and exists in Parquet format.\n",
    "Each Parquet file consists of 198 unique frames, with an image per camera (cameras 1 to 5), for a snippet of video.\n",
    "The frames are from vehicle motion, in a given city, at a given time of day, in given weather."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80cdfcb-d7aa-4fd8-8f09-2aad514bc691",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.parquet as pq\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import io\n",
    "import matplotlib.patches as patches\n",
    "import cv2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e3aae0-762c-4e77-a4cd-1dbc0e377fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_parquet_schema(local_path: str):\n",
    "    # Read the Parquet file\n",
    "    table = pq.read_table(local_path)\n",
    "    # View the schema (to find the image column)\n",
    "    print(\"Schema:\\n\", table.schema)\n",
    "    print(\"Columns:\\n\", table.column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a192ac6-1633-40ed-9418-8664453a2065",
   "metadata": {},
   "source": [
    "Once you gain access to the GCS bucket, download a Parquet file from the `camera_box` and `camera_image` folders. Ensure that they are named the same, so that they correspond to the same frame.\n",
    "Let's investigate the schema:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398407e6-4efb-44c9-ab92-05a270c84df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "camera_box_parquet = '/path/to/parquet'\n",
    "camera_image_parquet = '/path/to/parquet'\n",
    "\n",
    "print(\"=== Camera Box ===\")\n",
    "print_parquet_schema(camera_box_parquet)\n",
    "print(\"=== Camera Image ===\")\n",
    "print_parquet_schema(camera_image_parquet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dda733d-3867-477c-95c8-89910e236214",
   "metadata": {},
   "source": [
    "Now, let's overlay a frame from the camera_image parquet with the corresponding bounding box:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1d7ac6-ad13-448f-9006-eefe02d2d2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === STEP 1: Load the image file ===\n",
    "# Load Parquet tables\n",
    "camera_image_table = pq.read_table(camera_image_parquet)\n",
    "camera_box_table = pq.read_table(camera_box_parquet)\n",
    "\n",
    "# Inspect available columns\n",
    "print(\"Image columns:\", camera_image_table.column_names)\n",
    "print(\"Box columns:\", camera_box_table.column_names)\n",
    "\n",
    "# Pick one row from camera image\n",
    "image_row = camera_image_table.to_pandas().iloc[0]\n",
    "\n",
    "# Extract image bytes\n",
    "image_bytes = image_row['[CameraImageComponent].image']\n",
    "image = Image.open(io.BytesIO(image_bytes)).convert(\"RGB\")\n",
    "\n",
    "# Get keys for matching\n",
    "frame_timestamp = image_row['key.frame_timestamp_micros']\n",
    "camera_name = image_row['key.camera_name']\n",
    "\n",
    "# === STEP 2: Get matching bounding boxes for this image ===\n",
    "box_df = camera_box_table.to_pandas()\n",
    "\n",
    "# Filter boxes matching the same frame and camera\n",
    "matching_boxes = box_df[\n",
    "    (box_df['key.frame_timestamp_micros'] == frame_timestamp) &\n",
    "    (box_df['key.camera_name'] == camera_name)\n",
    "]\n",
    "\n",
    "# === STEP 3: Draw image and overlay boxes ===\n",
    "fig, ax = plt.subplots(1, figsize=(12, 8))\n",
    "ax.imshow(image)\n",
    "\n",
    "for _, box in matching_boxes.iterrows():\n",
    "    cx = box['[CameraBoxComponent].box.center.x']\n",
    "    cy = box['[CameraBoxComponent].box.center.y']\n",
    "    w = box['[CameraBoxComponent].box.size.x']\n",
    "    h = box['[CameraBoxComponent].box.size.y']\n",
    "    \n",
    "    # Convert from center to top-left\n",
    "    x1 = cx - w / 2\n",
    "    y1 = cy - h / 2\n",
    "\n",
    "    # Create a rectangle\n",
    "    rect = patches.Rectangle((x1, y1), w, h, linewidth=2,\n",
    "                             edgecolor='red', facecolor='none')\n",
    "    ax.add_patch(rect)\n",
    "\n",
    "# Optional: show camera name and frame time\n",
    "ax.set_title(f\"Camera: {camera_name} | Timestamp: {frame_timestamp}\")\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "# Example Waymo class label map\n",
    "label_map = {\n",
    "    0: \"Unknown\",\n",
    "    1: \"Vehicle\",\n",
    "    2: \"Pedestrian\",\n",
    "    3: \"Sign\",\n",
    "    4: \"Cyclist\"\n",
    "}\n",
    "\n",
    "# Print class info for each bounding box\n",
    "print(\"Bounding Box Classifications:\")\n",
    "for i, box in matching_boxes.iterrows():\n",
    "    class_id = box['[CameraBoxComponent].type']\n",
    "    class_name = label_map.get(class_id, f\"Unknown ({class_id})\")\n",
    "    print(f\"- Object {i}: Class ID = {class_id}, Label = {class_name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53026dc6-cb1b-4f1e-9cae-cb6b0029084d",
   "metadata": {},
   "source": [
    "Showcase contents of a Parquet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3495266-de7b-41af-a858-40016a20dee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "table = pq.read_table(camera_image_parquet)\n",
    "print(f\"Number of images: {table.num_rows}\")\n",
    "timestamps = table.column(\"key.frame_timestamp_micros\").to_pylist()\n",
    "print(\"Unique frames:\", len(set(timestamps)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0aca3e-5be8-4ebe-8643-7384f67f35b7",
   "metadata": {},
   "source": [
    "Can even show how a the Parquet is essentially a video made up of individual frames:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7977d8-9fab-4cf7-bd78-edb824a48909",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Step 1: Load the parquet files ===\n",
    "camera_image_table = pq.read_table(camera_image_parquet)\n",
    "camera_box_table = pq.read_table(camera_box_parquet)\n",
    "\n",
    "# Convert to DataFrames\n",
    "df_img = camera_image_table.to_pandas()\n",
    "df_box = camera_box_table.to_pandas()\n",
    "\n",
    "# === Step 2: Filter to one camera (e.g. FRONT == 1) ===\n",
    "CAMERA_ID = 1  # change if needed\n",
    "\n",
    "df_img = df_img[df_img['key.camera_name'] == CAMERA_ID]\n",
    "df_box = df_box[df_box['key.camera_name'] == CAMERA_ID]\n",
    "\n",
    "# === Step 3: Get the first 20 unique frames ===\n",
    "first_20_frames = df_img['key.frame_timestamp_micros'].unique()[:20]\n",
    "\n",
    "# === Step 4: Plot each frame with bounding boxes ===\n",
    "for i, timestamp in enumerate(first_20_frames):\n",
    "    img_row = df_img[df_img['key.frame_timestamp_micros'] == timestamp].iloc[0]\n",
    "    image_bytes = img_row['[CameraImageComponent].image']\n",
    "    image = Image.open(io.BytesIO(image_bytes)).convert(\"RGB\")\n",
    "\n",
    "    boxes = df_box[df_box['key.frame_timestamp_micros'] == timestamp]\n",
    "\n",
    "    # Plot image\n",
    "    fig, ax = plt.subplots(1, figsize=(10, 6))\n",
    "    ax.imshow(image)\n",
    "\n",
    "    for _, box in boxes.iterrows():\n",
    "        cx = box['[CameraBoxComponent].box.center.x']\n",
    "        cy = box['[CameraBoxComponent].box.center.y']\n",
    "        w = box['[CameraBoxComponent].box.size.x']\n",
    "        h = box['[CameraBoxComponent].box.size.y']\n",
    "        x1 = cx - w / 2\n",
    "        y1 = cy - h / 2\n",
    "\n",
    "        rect = patches.Rectangle((x1, y1), w, h, linewidth=2,\n",
    "                                 edgecolor='red', facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "\n",
    "    ax.set_title(f\"Frame {i+1} | Timestamp: {timestamp}\")\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c457a1-7cf7-46af-9ac7-24db81691ec4",
   "metadata": {},
   "source": [
    "Or even generate an actual video:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b552e28-156e-4e98-9237-7ff23dc46e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Load parquet files ===\n",
    "camera_image_table = pq.read_table(camera_image_parquet)\n",
    "camera_box_table = pq.read_table(camera_box_parquet)\n",
    "\n",
    "df_img = camera_image_table.to_pandas()\n",
    "df_box = camera_box_table.to_pandas()\n",
    "\n",
    "# === Filter to a single camera (e.g. FRONT) ===\n",
    "CAMERA_ID = 1  # FRONT camera, typically\n",
    "df_img = df_img[df_img['key.camera_name'] == CAMERA_ID]\n",
    "df_box = df_box[df_box['key.camera_name'] == CAMERA_ID]\n",
    "\n",
    "# === Sort by frame timestamp ===\n",
    "df_img = df_img.sort_values('key.frame_timestamp_micros')\n",
    "timestamps = df_img['key.frame_timestamp_micros'].unique()\n",
    "\n",
    "# === Label map from Waymo class IDs ===\n",
    "label_map = {\n",
    "    0: \"Unknown\",\n",
    "    1: \"Vehicle\",\n",
    "    2: \"Pedestrian\",\n",
    "    3: \"Sign\",\n",
    "    4: \"Cyclist\"\n",
    "}\n",
    "\n",
    "# === Color map for class IDs (BGR format for OpenCV) ===\n",
    "color_map = {\n",
    "    0: 128, 128, 128)     # Gray for Unknown\n",
    "    1: (0, 0, 255),       # Red for Vehicle\n",
    "    2: (0, 255, 0),       # Green for Pedestrian\n",
    "    3: (255, 0, 0),       # Blue for Sign\n",
    "    4: (0, 255, 255)     # Yellow for Cyclist\n",
    "}\n",
    "\n",
    "# === Get video frame size ===\n",
    "sample_image = Image.open(io.BytesIO(df_img.iloc[0]['[CameraImageComponent].image'])).convert(\"RGB\")\n",
    "video_width, video_height = sample_image.size\n",
    "\n",
    "# === Set up OpenCV video writer ===\n",
    "fps = 10\n",
    "video_writer = cv2.VideoWriter(\n",
    "    \"waymo_output_video.mp4\",\n",
    "    cv2.VideoWriter_fourcc(*'mp4v'),\n",
    "    fps,\n",
    "    (video_width, video_height)\n",
    ")\n",
    "\n",
    "# === Frame-by-frame processing ===\n",
    "for timestamp in timestamps:\n",
    "    # Load image\n",
    "    img_row = df_img[df_img['key.frame_timestamp_micros'] == timestamp].iloc[0]\n",
    "    image = Image.open(io.BytesIO(img_row['[CameraImageComponent].image'])).convert(\"RGB\")\n",
    "    img_np = np.array(image)\n",
    "\n",
    "    # Get boxes for this frame\n",
    "    boxes = df_box[df_box['key.frame_timestamp_micros'] == timestamp]\n",
    "\n",
    "    for _, box in boxes.iterrows():\n",
    "        cx = box['[CameraBoxComponent].box.center.x']\n",
    "        cy = box['[CameraBoxComponent].box.center.y']\n",
    "        w = box['[CameraBoxComponent].box.size.x']\n",
    "        h = box['[CameraBoxComponent].box.size.y']\n",
    "        x1 = int(cx - w / 2)\n",
    "        y1 = int(cy - h / 2)\n",
    "        x2 = int(cx + w / 2)\n",
    "        y2 = int(cy + h / 2)\n",
    "\n",
    "        class_id = box['[CameraBoxComponent].type']\n",
    "        class_name = label_map.get(class_id, f\"Unknown ({class_id})\")\n",
    "        color = color_map.get(class_id, (255, 255, 255))  # default white\n",
    "\n",
    "        # Draw bounding box\n",
    "        cv2.rectangle(img_np, (x1, y1), (x2, y2), color=color, thickness=2)\n",
    "\n",
    "        # Prepare label text\n",
    "        font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "        font_scale = 0.6\n",
    "        text_size = cv2.getTextSize(class_name, font, font_scale, 1)[0]\n",
    "        text_origin = (x1, max(y1 - 5, text_size[1]))\n",
    "\n",
    "        # Draw background for text\n",
    "        cv2.rectangle(img_np,\n",
    "                      (text_origin[0], text_origin[1] - text_size[1]),\n",
    "                      (text_origin[0] + text_size[0], text_origin[1] + 4),\n",
    "                      color, thickness=-1)\n",
    "\n",
    "        # Draw label\n",
    "        cv2.putText(img_np, class_name, (text_origin[0], text_origin[1]),\n",
    "                    font, font_scale, (0, 0, 0), thickness=1, lineType=cv2.LINE_AA)\n",
    "\n",
    "    # Convert to BGR for OpenCV\n",
    "    frame_bgr = cv2.cvtColor(img_np, cv2.COLOR_RGB2BGR)\n",
    "    video_writer.write(frame_bgr)\n",
    "\n",
    "# === Finalize video ===\n",
    "video_writer.release()\n",
    "print(\"Video saved as 'waymo_output_video.mp4'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3461f61-8106-4878-b66a-c065d811ee06",
   "metadata": {},
   "source": [
    "**Note: Steps to take if there was time:**\n",
    "\n",
    "Due to the vast size of the Waymo dataset, under tight time and compute resource constraints, it was not possible to examine the distribution of the image data - ie. the class distribution across all the samples.\n",
    "\n",
    "This would be useful for this use case, so under-represented classes (likely cyclists) can be idenitfied and potentially oversampled, so that the model performs better on these classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2f5eb8-5fa5-438e-b23f-0defdc9f30e4",
   "metadata": {},
   "source": [
    "## 2. Data Processing\n",
    "\n",
    "From this part of the notebook onwards, due to the compute and storage requirements for the large scale of image data required to train a Faster R-CNN model, a Google Cloud VM was (and should be) used for the compute with an 800 GB disk attached to store the images.\n",
    "\n",
    "You can use GCS to store the image data, and mount it on the VM using GCSFuse, but at the cost of slower data loading speeds when training.\n",
    "\n",
    "**Notes about Google Cloud:**\n",
    "- You can set up a free-tier Google Cloud account [here](https://cloud.google.com/free?hl=en)\n",
    "- This initial $300 is enough to get going with a VM and storage, but does not cover for any GPUs used for training.\n",
    "- Any cost incurred by using this notebook on Google Cloud is at the cost of the user.\n",
    "- Once set up on GCP, you will need to request a higher quota on the SDD storage on the VM.\n",
    "- When doing the data processing, ensure that you are authenticated as the email that you used to gain access to the Waymo data, otherwise you'll run into authentication errors. You can do this by running `gcloud auth login` on the VM and chosing the relevant Gmail email, and then the same for `gcloud auth application-default login`.\n",
    "\n",
    "In this section, we will be transforming the Parquet files into `.pt` files, ready to be used for training, validation and testing, and saving them to the disk storage on the VM for downstream uses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25244bb2-7bf9-4d91-b5b2-6d1fcc102001",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gcsfs\n",
    "import pyarrow.parquet as pq\n",
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "import io\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import logging\n",
    "import gc\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ========== CONFIG ==========\n",
    "SOURCE_BUCKET = \"waymo_open_dataset_v_2_0_1\"\n",
    "SPLIT = \"training\"\n",
    "NUM_BATCHES = 4               \n",
    "BATCH_ID = 0                  \n",
    "OUTPUT_ROOT = \"/tmp/waymo_data\"\n",
    "TARGET_PT_FILES = 18000      \n",
    "RANDOM_SEED = 42\n",
    "# ============================\n",
    "\n",
    "fs = gcsfs.GCSFileSystem(token=\"google_default\")\n",
    "\n",
    "# Logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(f\"BATCH {BATCH_ID}\")\n",
    "\n",
    "# Global counter\n",
    "global_pt_counter = 0\n",
    "\n",
    "\n",
    "def list_parquet_files(prefix):\n",
    "    files = fs.ls(prefix)\n",
    "    return sorted([os.path.basename(f) for f in files if f.endswith(\".parquet\")])\n",
    "\n",
    "def process_file(filename, max_to_save):\n",
    "    global global_pt_counter\n",
    "\n",
    "    if global_pt_counter >= max_to_save:\n",
    "        return 0\n",
    "\n",
    "    GCS_PREFIX = f\"{SOURCE_BUCKET}/{SPLIT}\"\n",
    "    image_path = f\"gs://{GCS_PREFIX}/camera_image/{filename}\"\n",
    "    box_path = f\"gs://{GCS_PREFIX}/camera_box/{filename}\"\n",
    "    stats_path = f\"gs://{GCS_PREFIX}/stats/{filename}\"\n",
    "    output_dir = os.path.join(OUTPUT_ROOT, SPLIT)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    if not fs.exists(box_path[5:]) or not fs.exists(stats_path[5:]):\n",
    "        print(f\"Missing box or stats for {filename}, skipping.\")\n",
    "        return 0\n",
    "\n",
    "    try:\n",
    "        with fs.open(image_path, 'rb') as f_img:\n",
    "            df_img = pq.read_table(f_img).to_pandas()\n",
    "        with fs.open(box_path, 'rb') as f_box:\n",
    "            df_box = pq.read_table(f_box).to_pandas()\n",
    "        with fs.open(stats_path, 'rb') as f_stats:\n",
    "            df_stats = pq.read_table(f_stats).to_pandas()\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to read {filename}: {e}\")\n",
    "        return 0\n",
    "\n",
    "    df_stats = df_stats[['key.frame_timestamp_micros', '[StatsComponent].location',\n",
    "                         '[StatsComponent].time_of_day', '[StatsComponent].weather']]\n",
    "    pairs = df_img[['key.frame_timestamp_micros', 'key.camera_name']].drop_duplicates()\n",
    "\n",
    "    processed = 0\n",
    "\n",
    "    for _, row in pairs.iterrows():\n",
    "        if global_pt_counter >= max_to_save:\n",
    "            break\n",
    "\n",
    "        timestamp = row['key.frame_timestamp_micros']\n",
    "        camera_id = row['key.camera_name']\n",
    "        output_path = os.path.join(output_dir, f\"frame_{timestamp}_{camera_id}.pt\")\n",
    "\n",
    "        if os.path.exists(output_path):\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            img_row = df_img[\n",
    "                (df_img['key.frame_timestamp_micros'] == timestamp) &\n",
    "                (df_img['key.camera_name'] == camera_id)\n",
    "            ].iloc[0]\n",
    "\n",
    "            image = Image.open(io.BytesIO(img_row['[CameraImageComponent].image'])).convert(\"RGB\")\n",
    "            image_tensor = torch.from_numpy(np.array(image)).permute(2, 0, 1).float() / 255.0\n",
    "\n",
    "            boxes_df = df_box[\n",
    "                (df_box['key.frame_timestamp_micros'] == timestamp) &\n",
    "                (df_box['key.camera_name'] == camera_id)\n",
    "            ]\n",
    "\n",
    "            box_list, label_list = [], []\n",
    "            for _, box in boxes_df.iterrows():\n",
    "                cx = box['[CameraBoxComponent].box.center.x']\n",
    "                cy = box['[CameraBoxComponent].box.center.y']\n",
    "                w = box['[CameraBoxComponent].box.size.x']\n",
    "                h = box['[CameraBoxComponent].box.size.y']\n",
    "                x1, y1, x2, y2 = cx - w / 2, cy - h / 2, cx + w / 2, cy + h / 2\n",
    "                box_list.append([x1, y1, x2, y2])\n",
    "                label_list.append(box['[CameraBoxComponent].type'])\n",
    "\n",
    "            stats_row = df_stats[df_stats['key.frame_timestamp_micros'] == timestamp]\n",
    "            if stats_row.empty:\n",
    "                continue\n",
    "\n",
    "            meta = {\n",
    "                \"timestamp\": int(timestamp),\n",
    "                \"camera_name\": int(camera_id),\n",
    "                \"location\": stats_row['[StatsComponent].location'].values[0],\n",
    "                \"time_of_day\": stats_row['[StatsComponent].time_of_day'].values[0],\n",
    "                \"weather\": stats_row['[StatsComponent].weather'].values[0],\n",
    "                \"split\": SPLIT,\n",
    "                \"source_file\": filename\n",
    "            }\n",
    "\n",
    "            sample = {\n",
    "                \"image\": image_tensor,\n",
    "                \"boxes\": torch.tensor(box_list, dtype=torch.float32),\n",
    "                \"labels\": torch.tensor(label_list, dtype=torch.int64),\n",
    "                \"meta\": meta\n",
    "            }\n",
    "\n",
    "            torch.save(sample, output_path)\n",
    "            processed += 1\n",
    "            global_pt_counter += 1\n",
    "\n",
    "            del image_tensor, sample, image, img_row, boxes_df, box_list, label_list, stats_row\n",
    "            gc.collect()\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing frame {timestamp} cam {camera_id}: {e}\")\n",
    "\n",
    "    del df_img, df_box, df_stats\n",
    "    gc.collect()\n",
    "\n",
    "    print(f\"Finished {filename}, saved {processed} samples.\")\n",
    "    return processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beea4226-49b0-4eb7-921d-a1868ff80100",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List and shuffle all files\n",
    "all_files = list_parquet_files(f\"{SOURCE_BUCKET}/{SPLIT}/camera_image\")\n",
    "\n",
    "random.seed(RANDOM_SEED)\n",
    "random.shuffle(all_files)\n",
    "\n",
    "# Simulate batching\n",
    "batch_files = [f for i, f in enumerate(all_files) if i % NUM_BATCHES == BATCH_ID]\n",
    "\n",
    "print(f\"Batch {BATCH_ID}: {len(batch_files)} files to process\")\n",
    "\n",
    "# Run batch loop\n",
    "for fname in tqdm(batch_files):\n",
    "    if global_pt_counter >= TARGET_PT_FILES:\n",
    "        break\n",
    "    process_file(fname, TARGET_PT_FILES)\n",
    "\n",
    "print(f\"Batch {BATCH_ID} complete. Total .pt files saved: {global_pt_counter}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f09eef-d517-404b-bed8-9227b106d6b8",
   "metadata": {},
   "source": [
    "The above code is an example of how the data can be processed from Parquet into `.pt`, in batches and saved into a /tmp directory in the VM.\n",
    "\n",
    "To run through all the batches, increase the value of `BATCH_ID` by 1 and run again.\n",
    "\n",
    "The Waymo Dataset is alreayd split into `training`, `validation` and `testing`. So you can change the value of `SPLIT` here to use the same code for the other sets.\n",
    "\n",
    "To fit onto 800GB of storage, 18,000 training samples, 5,000 validation samples and 4,000 testing samples will fit.\n",
    "\n",
    "**NOTE:** Because Waymo data is used in public competitions, to ensure no cheating in these competitions, the `testing` dataset actually doesn't come with any labels ie. bounding boxes. Therefore, you should re-use the validation set and save it to a `testing` directory. You should change the random seed when running the `testing` processing, as the same seed would duplicate files from your `validation` dataset into `testing`.\n",
    "\n",
    "**Running Batch Processing in Parallel**\n",
    "\n",
    "If you would like a script to use instead, please find it under `scripts/process_batch.py`. This can be used along with `run_batch.sh` to launch multiple processing jobs at once on a GCP VM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b9c2ca5-d0d3-42a1-a0b8-10520f8d5bec",
   "metadata": {},
   "source": [
    "### Improvement Note:\n",
    "The data taken and processed here is taken at random. This is likely to mimic the class distribution of the Waymo dataset, but this may not be favourable for under-represented classes eg. cyclists. This data processing script could/should be modified to balance the distributions of the classes, which would lead to better accuracy/precision for all classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5221ca-8b3a-44ef-ae0e-3989b40ba54f",
   "metadata": {},
   "source": [
    "## 3. Use Bayesian Optimisation to Find Optimal Hyperparameters\n",
    "\n",
    "As I have learned all about during this course, Bayesian Optimisation is very useful for black-box optimisation problems - especially ones where feedback is slow, like in hyperparameter tuning.\n",
    "\n",
    "We use Optuna to perform this BO task.\n",
    "\n",
    "Optuna is an open-source, Python-based hyperparameter optimization framework that helps you automatically search for the best hyperparameter values for your machine learning models.\n",
    "\n",
    "It is flexible, scalable, and often used for tasks like:\n",
    "- Finding the best learning rate, dropout, or architecture settings\n",
    "- Optimizing data preprocessing parameters\n",
    "- Tuning black-box functions where manual tuning would be slow or infeasible\n",
    "\n",
    "Optuna’s default algorithm is Tree-structured Parzen Estimator (TPE), which is a form of Bayesian optimization.\n",
    "- TPE models the performance of hyperparameters and uses this to balance exploration vs. exploitation.\n",
    "- Unlike classic grid/random search, Bayesian methods build a probabilistic model of the objective function and use it to choose the next promising hyperparameter set.\n",
    "\n",
    "### NOTE:\n",
    "The hyperparameter tuning, and the final training job will take __forever__ if done without a GPU.\n",
    "As mentioned before, GPUs are covered in the free-tier of GCP. I used one, because without it, I would still be waiting.\n",
    "I can't endorse that anyone else uses a GPU, but it does improve speed significantly.\n",
    "\n",
    "I used an L4 GPU VM (g2-standard-12 on Google Cloud)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2267757a-2f4c-433d-8fc3-9dc3a613e6ef",
   "metadata": {},
   "source": [
    "### First, let's define our custom Waymo DataLoader:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ab11b1-32be-4677-8408-543155733e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "\n",
    "class WaymoDataset(Dataset):\n",
    "    def __init__(self, gcs_prefix, file_list, transform=None, label_map=None):\n",
    "        self.gcs_prefix = gcs_prefix.rstrip('/')\n",
    "        self.file_list = file_list\n",
    "        self.transform = transform\n",
    "        self.label_map = label_map\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        for _ in range(len(self.file_list)):\n",
    "            file_name = self.file_list[idx]\n",
    "            file_path = os.path.join(self.gcs_prefix, file_name)\n",
    "\n",
    "            try:\n",
    "                with open(file_path, 'rb') as f:\n",
    "                    sample = torch.load(f)\n",
    "            except Exception as e:\n",
    "                raise RuntimeError(f\"Failed to load file: {file_path}, error: {e}\")\n",
    "\n",
    "            image = sample['image']\n",
    "            boxes = sample['boxes']\n",
    "            labels = sample['labels']\n",
    "\n",
    "            # Skip frames with no boxes\n",
    "            if boxes.shape[0] == 0:\n",
    "                idx = (idx + 1) % len(self.file_list)\n",
    "                continue\n",
    "\n",
    "            if self.label_map is not None:\n",
    "                labels = torch.tensor([self.label_map.get(int(lbl), 0) for lbl in labels], dtype=torch.int64)\n",
    "\n",
    "            target = {\n",
    "                'boxes': boxes,\n",
    "                'labels': labels\n",
    "            }\n",
    "\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "\n",
    "            return image, target\n",
    "\n",
    "        raise RuntimeError(\"All samples in dataset have no boxes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5097ceec-6c8a-4826-a168-606ccc0f7b9d",
   "metadata": {},
   "source": [
    "### Loading in the Pre-Trained Faster R-CNN model\n",
    "\n",
    "Although we were taught in this course to build CNNs from scratch, due to the complexity of the ML problem, and the tight time constraints, this was not feasible. However, it is very common practice to use a pre-trained model as the starting point.\n",
    "\n",
    "The pre-trained Faster R-CNN model with a ResNet-50 backbone and Feature Pyramid Network (FPN) is commonly used as a starting point for object detection tasks. Trained on the COCO dataset, it learns general visual features like edges, textures, and object shapes, which can transfer well to new datasets. In practice, the model is fine-tuned by replacing its classification head with a new one that matches the number of classes in the target dataset. This approach allows efficient adaptation to custom detection problems without training from scratch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c903b3a4-7023-471d-8a6e-56e1e0f7f818",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "\n",
    "def get_model(num_classes=4):  # 3 foreground classes + 1 background\n",
    "    # Load a pre-trained Faster R-CNN model with a ResNet-50 backbone and FPN\n",
    "    model = fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "\n",
    "    # Replace the classification head with one for our dataset\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c208b89e-6371-4d7f-a4a7-dc164099a350",
   "metadata": {},
   "source": [
    "Let's declare some utils:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8dfaf1-5a53-492a-9f17-fd07b63f855a",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {\n",
    "    1: 1,  # vehicle\n",
    "    2: 2,  # pedestrian\n",
    "    4: 3   # cyclist\n",
    "}\n",
    "# Note that Waymo says cyclists are a 4, because 3 was used for signs, even though there are no sign labels in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd32fcf2-5ec9-4742-ae10-642c12386548",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(model, optimizer, epoch, path):\n",
    "    \"\"\"\n",
    "    Save model checkpoint to a path in the mounted GCSFuse directory.\n",
    "    \"\"\"\n",
    "    checkpoint = {\n",
    "        'model_state': model.state_dict(),\n",
    "        'optimizer_state': optimizer.state_dict(),\n",
    "        'epoch': epoch\n",
    "    }\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    torch.save(checkpoint, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4769ee20-1b18-416e-8e16-8f78e4d9a9af",
   "metadata": {},
   "source": [
    "### Define our training loop for the hyperparameter experiments:\n",
    "\n",
    "This is used in the experiments to find the best combination of hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154ca48c-8ce8-482a-9d66-f683ecf11c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "def list_local_files(directory):\n",
    "    return sorted([f for f in os.listdir(directory) if f.endswith('.pt')])\n",
    "\n",
    "def train_one_epoch(model, dataloader, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch_idx, (images, targets) in enumerate(dataloader):\n",
    "        images = [img.to(device) for img in images]\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        loss_dict = model(images, targets)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += losses.item()\n",
    "\n",
    "        now = datetime.now()\n",
    "        print(f\"Time: {now.strftime('%H:%M:%S')} - [Train] Batch {batch_idx + 1}/{len(dataloader)} - Loss: {losses.item():.4f}\")\n",
    "\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def validate(model, dataloader, device):\n",
    "    model.eval()\n",
    "    metric = MeanAveragePrecision(iou_thresholds=[0.5])\n",
    "    with torch.no_grad():\n",
    "        for images, targets in dataloader:\n",
    "            images = [img.to(device) for img in images]\n",
    "            targets_cpu = [{k: v.cpu() for k, v in t.items()} for t in targets]\n",
    "\n",
    "            outputs = model(images)\n",
    "            outputs_cpu = [{k: v.cpu() for k, v in o.items()} for o in outputs]\n",
    "\n",
    "            metric.update(outputs_cpu, targets_cpu)\n",
    "\n",
    "    score = metric.compute()\n",
    "    map_50 = score['map_50'].item()\n",
    "    print(f\"[Validation] mAP@0.5: {map_50:.4f}\")\n",
    "    return map_50\n",
    "\n",
    "def run_training(config):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = get_model(num_classes=4)  # 3 classes + background\n",
    "    model.to(device)\n",
    "\n",
    "    optimizer = torch.optim.SGD(\n",
    "        model.parameters(),\n",
    "        lr=config[\"lr\"],\n",
    "        momentum=config[\"momentum\"],\n",
    "        weight_decay=config[\"weight_decay\"]\n",
    "    )\n",
    "\n",
    "    train_files = list_local_files(config[\"train_prefix\"])\n",
    "    val_files = list_local_files(config[\"val_prefix\"])\n",
    "    train_files = train_files[:config[\"num_train_files\"]]\n",
    "    val_files = val_files[:config[\"num_val_files\"]]\n",
    "\n",
    "    label_map = label_map\n",
    "    train_dataset = WaymoDataset(config[\"train_prefix\"], train_files, label_map=label_map)\n",
    "    val_dataset = WaymoDataset(config[\"val_prefix\"], val_files, label_map=label_map)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=config[\"batch_size\"], shuffle=True,\n",
    "                              collate_fn=lambda x: tuple(zip(*x)), num_workers=8, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=config[\"batch_size\"], shuffle=False,\n",
    "                            collate_fn=lambda x: tuple(zip(*x)), num_workers=4, pin_memory=True)\n",
    "\n",
    "    best_map = 0.0\n",
    "\n",
    "    for epoch in range(config[\"epochs\"]):\n",
    "        print(f\"Starting Epoch: {epoch + 1}/{config['epochs']}\")\n",
    "        train_loss = train_one_epoch(model, train_loader, optimizer, device)\n",
    "        val_map = validate(model, val_loader, device)\n",
    "\n",
    "        if val_map > best_map:\n",
    "            best_map = val_map\n",
    "            save_checkpoint(model, optimizer, epoch, config[\"checkpoint_path\"])\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}: Train Loss = {train_loss:.4f}, Val mAP@0.5 = {val_map:.4f}\")\n",
    "\n",
    "    return best_map\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c248f7-158e-42bc-9265-dd8d054007dd",
   "metadata": {},
   "source": [
    "Declare the objective function for Optuna:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75826fe0-eb0a-4e8e-907f-4a6ad8e20fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    config = {\n",
    "        \"lr\": trial.suggest_loguniform(\"lr\", 1e-5, 1e-2),\n",
    "        \"momentum\": trial.suggest_float(\"momentum\", 0.7, 0.99),\n",
    "        \"weight_decay\": trial.suggest_loguniform(\"weight_decay\", 1e-6, 1e-3),\n",
    "        \"batch_size\": trial.suggest_categorical(\"batch_size\", [2, 4, 8]),\n",
    "        \"epochs\": 4,\n",
    "        \"train_prefix\": \"/tmp/waymo_data/training\",\n",
    "        \"val_prefix\": \"/tmp/waymo_data/validation\",\n",
    "        \"checkpoint_path\": \"/tmp/waymo_data/checkpoints/best_model.pt\",\n",
    "        \"num_train_files\": 6000,\n",
    "        \"num_val_files\": 2000\n",
    "    }\n",
    "\n",
    "    return run_training(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46b01b4-d3c4-46b3-ac25-a571bf71bed5",
   "metadata": {},
   "source": [
    "This will run 10 trials, using Optuna to find the best hyperparameter combination based on mAP@50. There is a timeout too, so whichever comes first: n_trials or timeout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b9e426-7461-46d2-91bc-a1a0aeefd592",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "\n",
    "n_trials=10\n",
    "timeout=36000\n",
    "\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(lambda trial: objective(trial), n_trials=n_trials, timeout=timeout)\n",
    "\n",
    "print(\"Best trial:\")\n",
    "print(study.best_trial)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2ee87d-522e-4aad-9dcc-1b3d28b71156",
   "metadata": {},
   "source": [
    "### Improvement Note:\n",
    "\n",
    "If I had more time, I would have let it run for longer with more experiements, in order to have a better chance at finding the most optimal set of hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5978a85b-61d6-4363-a1a0-981c280f3682",
   "metadata": {},
   "source": [
    "## 4. Training of Final Model\n",
    "\n",
    "Now that we have our optimal hyperparameters, time to plug them into the final training job.\n",
    "This training job will use the whole training dataset from earlier.\n",
    "\n",
    "This training function is the same, only it has early-stopping logic, so if there is no improvement for X continuous epochs, then the training stops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da77f79-445f-405b-b30d-e5fb05adde35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_final_training(config):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = get_model(num_classes=4)  # 3 classes + background\n",
    "    model.to(device)\n",
    "\n",
    "    optimizer = torch.optim.SGD(\n",
    "        model.parameters(),\n",
    "        lr=config[\"lr\"],\n",
    "        momentum=config[\"momentum\"],\n",
    "        weight_decay=config[\"weight_decay\"]\n",
    "    )\n",
    "\n",
    "    train_files = list_local_files(config[\"train_prefix\"])\n",
    "    val_files = list_local_files(config[\"val_prefix\"])\n",
    "    train_files = train_files[:config[\"num_train_files\"]]\n",
    "    val_files = val_files[:config[\"num_val_files\"]]\n",
    "\n",
    "    label_map = label_map\n",
    "    train_dataset = WaymoDataset(config[\"train_prefix\"], train_files, label_map=label_map)\n",
    "    val_dataset = WaymoDataset(config[\"val_prefix\"], val_files, label_map=label_map)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=config[\"batch_size\"], shuffle=True,\n",
    "                              collate_fn=lambda x: tuple(zip(*x)), num_workers=8, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=config[\"batch_size\"], shuffle=False,\n",
    "                            collate_fn=lambda x: tuple(zip(*x)), num_workers=4, pin_memory=True)\n",
    "\n",
    "    best_map = 0.0\n",
    "    best_epoch = 0\n",
    "    patience = config.get(\"early_stopping_patience\", 5)\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    for epoch in range(config[\"epochs\"]):\n",
    "        print(f\"Starting Epoch: {epoch + 1}/{config['epochs']}\")\n",
    "        train_loss = train_one_epoch(model, train_loader, optimizer, device)\n",
    "        val_map = validate(model, val_loader, device)\n",
    "\n",
    "        if val_map > best_map:\n",
    "            best_map = val_map\n",
    "            best_epoch = epoch\n",
    "            save_checkpoint(model, optimizer, epoch, config[\"checkpoint_path\"])\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}: Train Loss = {train_loss:.4f}, Val mAP@0.5 = {val_map:.4f}\")\n",
    "\n",
    "        if epochs_no_improve >= patience:\n",
    "            print(f\"Early stopping triggered at epoch {epoch + 1}\")\n",
    "            break\n",
    "\n",
    "    print(f\"Best model at epoch {best_epoch + 1} with mAP@0.5 = {best_map:.4f}\")\n",
    "    return best_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5ae877-d38e-433c-a9e6-600bfad6805c",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_config = {\n",
    "    \"lr\": <ADD OPTIMAL LR HERE>,\n",
    "    \"momentum\": <ADD OPTIMAL MOMENTUM HERE>,\n",
    "    \"weight_decay\": <ADD OPTIMAL WEIGHT DECAY HERE>,\n",
    "    \"batch_size\": <ADD OPTIMAL BATCH SIZE HERE>,\n",
    "    \"epochs\": 30,\n",
    "    \"checkpoint_path\": \"/tmp/waymo_data/checkpoints/final/best_model.pt\",\n",
    "    \"train_prefix\": \"/tmp/waymo_data/training\",\n",
    "    \"val_prefix\": \"/tmp/waymo_data/validation\",\n",
    "    \"test_prefix\": \"/tmp/waymo_data/testing\",\n",
    "    \"num_train_files\": 18000, #check you have this many files first\n",
    "    \"num_val_files\": 5000, #check you have this many files first\n",
    "    \"num_test_files\": 4000, #check you have this many files first\n",
    "    \"early_stopping_patience\": 5, # if no improvement for 5 consecutive epochs, stop training early.\n",
    "}\n",
    "\n",
    "# Train\n",
    "run_final_training(final_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4a5981-4570-4237-a3ba-734657af7452",
   "metadata": {},
   "source": [
    "## 5. Evaluate Final Model\n",
    "\n",
    "Run the model against the testing dataset to see how it performs!\n",
    "\n",
    "This runs the evalutation using torchmetrics MeanAveragePrecision and plots a graph of per-class mAP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63959cdf-4920-4ca5-ab26-a57f41c1f6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(checkpoint_path, test_prefix, num_test_files=1000, batch_size=4):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    print(\"Loading best model from checkpoint...\")\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    model = get_model(num_classes=4)\n",
    "    model.load_state_dict(checkpoint[\"model_state\"])\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    test_files = list_local_files(test_prefix)\n",
    "    test_files = test_files[:num_test_files]\n",
    "\n",
    "    label_map = label_map\n",
    "    test_dataset = WaymoDataset(test_prefix, test_files, label_map=label_map)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False,\n",
    "                             collate_fn=lambda x: tuple(zip(*x)), num_workers=4, pin_memory=True)\n",
    "\n",
    "    print(\"Runing Testing:\")\n",
    "    metric = MeanAveragePrecision(class_metrics=True)\n",
    "    with torch.no_grad():\n",
    "        for images, targets in test_loader:\n",
    "            images = [img.to(device) for img in images]\n",
    "            targets_cpu = [{k: v.cpu() for k, v in t.items()} for t in targets]\n",
    "            outputs = model(images)\n",
    "            outputs_cpu = [{k: v.cpu() for k, v in o.items()} for o in outputs]\n",
    "            metric.update(outputs_cpu, targets_cpu)\n",
    "\n",
    "    results = metric.compute()\n",
    "    print(\"Final Test Set Evaluation:\")\n",
    "    for k, v in results.items():\n",
    "        if isinstance(v, torch.Tensor):\n",
    "            if v.ndim == 0:\n",
    "                print(f\"{k}: {v.item():.4f}\")\n",
    "            else:\n",
    "                print(f\"{k}: {v}\")\n",
    "        else:\n",
    "            print(f\"{k}: {v}\")\n",
    "\n",
    "\n",
    "    # Plot per-class AP\n",
    "    if \"classes\" in results and \"map_per_class\" in results:\n",
    "        class_ids = list(range(len(results[\"map_per_class\"])))\n",
    "        ap_values = results[\"map_per_class\"].tolist()\n",
    "\n",
    "        plt.figure(figsize=(8, 5))\n",
    "        plt.bar(class_ids, ap_values)\n",
    "        plt.xlabel(\"Class ID\")\n",
    "        plt.ylabel(\"AP\")\n",
    "        plt.title(\"Per-Class Average Precision (AP)\")\n",
    "        plt.savefig(\"per_class_ap.png\")\n",
    "        print(\"Saved per-class AP plot to per_class_ap.png\")\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c46273-b73b-4f6f-9d5f-7fcb2d2d4ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model(\n",
    "    checkpoint_path=final_config[\"checkpoint_path\"],\n",
    "    test_prefix=final_config[\"test_prefix\"],\n",
    "    num_test_files=final_config[\"num_test_files\"],\n",
    "    batch_size=final_config[\"batch_size\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7169ef5-00d8-4309-8000-71a0c361bd1a",
   "metadata": {},
   "source": [
    "And this will give you some metrics, eg. for my training and evaluation:\n",
    "\n",
    "Final Test Set Evaluation:\n",
    "- map: 0.3047\n",
    "- map_50: 0.5129\n",
    "- map_75: 0.3142\n",
    "- map_small: 0.0564\n",
    "- map_medium: 0.2959\n",
    "- map_large: 0.5829\n",
    "- mar_1: 0.1566\n",
    "- mar_10: 0.3460\n",
    "- mar_100: 0.3959\n",
    "- mar_small: 0.1212\n",
    "- mar_medium: 0.4199\n",
    "- mar_large: 0.6822\n",
    "- map_per_class: tensor([0.3848, 0.2978, 0.2314])\n",
    "- mar_100_per_class: tensor([0.4387, 0.3537, 0.3952])\n",
    "- classes: tensor([1, 2, 3], dtype=torch.int32)\n",
    "\n",
    "And will also give you a png chart of your per-class mAP."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597ae937-ca0b-4bed-8d42-98ef46285af8",
   "metadata": {},
   "source": [
    "## Final Notes:\n",
    "\n",
    "- The biggest bottlenecks here are time and money. It is totally possible to attach a 100 TB Parallelstore volume to a multi-GPU VM and do the training over all the training data provided by Waymo. It would just be very expensive, and still take a long time to run hyperparameter experiments and train the final model.\n",
    "- On the topic of time bottlenecks, the biggest factor here is actually the speed at which you can load samples in during the training process. Optimizations here would be very impactful. Distributed training would certainly help.\n",
    "\n",
    "### Final, Final Note:\n",
    "If you used any GCP resources, delete, destroy and wipe before you get charged :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45d8842-9ff8-482d-9f24-0e38312c526b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (Waymo)",
   "language": "python",
   "name": "waymo311"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
